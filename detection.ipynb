{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import errno\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "import collections\n",
    "import multiprocessing\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import utils\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import faster_rcnn\n",
    "from faster_rcnn.rpn import AnchorGenerator\n",
    "from faster_rcnn.transform import GeneralizedRCNNTransform\n",
    "from faster_rcnn.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def get_transform(train: bool, predict_only=False) -> Callable:\n",
    "    train_initial_size = 2048\n",
    "    crop_min_max_height = (400, 533)\n",
    "    crop_width = 512\n",
    "    crop_height = 384\n",
    "    if train:\n",
    "        transforms = [\n",
    "            A.LongestMaxSize(max_size=train_initial_size),\n",
    "            A.RandomSizedCrop(\n",
    "                min_max_height=crop_min_max_height,\n",
    "                width=crop_width,\n",
    "                height=crop_height,\n",
    "                w2h_ratio=crop_width / crop_height,\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=7,\n",
    "                sat_shift_limit=10,\n",
    "                val_shift_limit=10,\n",
    "            ),\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.RandomGamma(),\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        test_size = int(train_initial_size *\n",
    "                        crop_height / np.mean(crop_min_max_height))\n",
    "        print(f'Test image max size {test_size} px')\n",
    "        transforms = [\n",
    "            A.LongestMaxSize(max_size=test_size),\n",
    "        ]\n",
    "    transforms.extend([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    if predict_only:\n",
    "        return A.Compose(\n",
    "            transforms\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            transforms,\n",
    "            bbox_params={\n",
    "                'format': 'coco',\n",
    "                'min_area': 0,\n",
    "                'min_visibility': 0.5,\n",
    "                'label_fields': ['labels'],\n",
    "            },\n",
    "        )\n",
    "\n",
    "class ModelTransform(GeneralizedRCNNTransform):\n",
    "    def __init__(self, image_mean, image_std):\n",
    "        nn.Module.__init__(self)\n",
    "        self.image_mean = image_mean\n",
    "        self.image_std = image_std\n",
    "\n",
    "    def resize(self, image, target):\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('input/train.csv')\n",
    "\n",
    "df_train, df_val = train_test_split(df_all, test_size=0.2, random_state=42)\n",
    "\n",
    "df_test = pd.read_csv('input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>100249416_00017_2</td>\n",
       "      <td>U+4E00 517 433 71 11 U+4E00 1163 432 75 11 U+4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>200015779_00087_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>200015779_00043_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>200021660-00080_2</td>\n",
       "      <td>U+4E00 337 936 97 21 U+3078 1424 3704 103 36 U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>200021763-00043_1</td>\n",
       "      <td>U+30CF 1143 3033 92 35 U+30CF 501 804 60 37 U+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               image_id                                             labels\n",
       "278   100249416_00017_2  U+4E00 517 433 71 11 U+4E00 1163 432 75 11 U+4...\n",
       "1598  200015779_00087_1                                                NaN\n",
       "1532  200015779_00043_1                                                NaN\n",
       "2011  200021660-00080_2  U+4E00 337 936 97 21 U+3078 1424 3704 103 36 U...\n",
       "2282  200021763-00043_1  U+30CF 1143 3033 92 35 U+30CF 501 804 60 37 U+..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3104, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>umgy001-048</td>\n",
       "      <td>U+304B 1107 2641 43 32 U+30CB 709 1742 50 33 U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>200004148_00035_1</td>\n",
       "      <td>U+4E00 837 1831 74 13 U+306F 564 2233 44 28 U+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>hnsd010-016</td>\n",
       "      <td>U+30CB 274 920 56 25 U+30FD 1077 2118 27 29 U+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>umgy006-020</td>\n",
       "      <td>U+309D 689 2295 45 43 U+309D 311 2063 34 48 U+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>200005598_00044_1</td>\n",
       "      <td>U+30FD 601 2300 27 40 U+3044 969 952 65 45 U+3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               image_id                                             labels\n",
       "3431        umgy001-048  U+304B 1107 2641 43 32 U+30CB 709 1742 50 33 U...\n",
       "1018  200004148_00035_1  U+4E00 837 1831 74 13 U+306F 564 2233 44 28 U+...\n",
       "3270        hnsd010-016  U+30CB 274 920 56 25 U+30FD 1077 2118 27 29 U+...\n",
       "3613        umgy006-020  U+309D 689 2295 45 43 U+309D 311 2063 34 48 U+...\n",
       "1200  200005598_00044_1  U+30FD 601 2300 27 40 U+3044 969 952 65 45 U+3..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(777, 2)\n"
     ]
    }
   ],
   "source": [
    "display(df_train.head())\n",
    "print(df_train.shape)\n",
    "\n",
    "display(df_val.head())\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def get_target_boxes_labels(item):\n",
    "    if item.labels and type(item.labels) is str:\n",
    "        labels = np.array(item.labels.split(' ')).reshape(-1, 5)\n",
    "    else:\n",
    "        labels = np.zeros((0, 5))\n",
    "    boxes = labels[:, 1:].astype(np.float)\n",
    "    labels = labels[:, 0]\n",
    "    return boxes, labels\n",
    "\n",
    "def get_image_path(item, root: Path = None) -> Path:\n",
    "    path = root + '/' + f'{item.image_id}.jpg'\n",
    "    return path\n",
    "\n",
    "def read_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform: Callable, root: Path,\n",
    "                 skip_empty: bool, is_train = True):\n",
    "        self.df = df\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.skip_empty = skip_empty\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        image = read_image(get_image_path(item, self.root))\n",
    "        h, w, _ = image.shape\n",
    "        if self.is_train:\n",
    "            bboxes, labels = get_target_boxes_labels(item)\n",
    "            # clip bboxes (else albumentations fails)\n",
    "            bboxes[:, 2] = (np.minimum(bboxes[:, 0] + bboxes[:, 2], w)\n",
    "                            - bboxes[:, 0])\n",
    "            bboxes[:, 3] = (np.minimum(bboxes[:, 1] + bboxes[:, 3], h)\n",
    "                            - bboxes[:, 1])\n",
    "            xy = {\n",
    "                'image': image,\n",
    "                'bboxes': bboxes.tolist(),\n",
    "                'labels': np.ones_like(labels, dtype=np.long),\n",
    "            }\n",
    "            xy = self.transform(**xy)\n",
    "            if not xy['bboxes'] and self.skip_empty:\n",
    "                return self[random.randint(0, len(self.df) - 1)]\n",
    "            image = xy['image']\n",
    "            boxes = torch.tensor(xy['bboxes']).reshape((len(xy['bboxes']), 4))\n",
    "            # convert to pytorch detection format\n",
    "            boxes[:, 2] += boxes[:, 0]\n",
    "            boxes[:, 3] += boxes[:, 1]\n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': torch.tensor(xy['labels'], dtype=torch.long),\n",
    "                'idx': torch.tensor(idx),\n",
    "            }\n",
    "            return image, target\n",
    "        else:\n",
    "            xy = {\n",
    "                'image': image,\n",
    "                'labels': np.ones_like(1, dtype=np.long),\n",
    "            }\n",
    "            xy = self.transform(**xy)\n",
    "            image = xy['image']\n",
    "            target = {\n",
    "                'idx': torch.tensor(idx),\n",
    "            }\n",
    "            return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(name: str, pretrained: bool, nms_threshold: float):\n",
    "    anchor_sizes = [12, 24, 32, 64, 96]\n",
    "    model = faster_rcnn.__dict__[name](\n",
    "        pretrained=pretrained,\n",
    "        rpn_anchor_generator=AnchorGenerator(\n",
    "            sizes=tuple((s,) for s in anchor_sizes),\n",
    "            aspect_ratios=tuple((0.5, 1.0, 2.0) for _ in anchor_sizes),\n",
    "        ),\n",
    "        box_detections_per_img=1000,\n",
    "        box_nms_thresh=nms_threshold,\n",
    "    )\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "        in_channels=model.roi_heads.box_predictor.cls_score.in_features,\n",
    "        num_classes=2)\n",
    "    model.transform = ModelTransform(\n",
    "        image_mean=model.transform.image_mean,\n",
    "        image_std=model.transform.image_std,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model('fasterrcnn_resnet152_fpn', pretrained=0, nms_threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(results):\n",
    "    tp = int(sum([x['tp'] for x in results]))\n",
    "    fp = int(sum([x['fp'] for x in results]))\n",
    "    fn = int(sum([x['fn'] for x in results]))\n",
    "    if (tp + fp) == 0 or (tp + fn) == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        if precision > 0 and recall > 0:\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "    return {'f1': float(f1), 'tp': tp, 'fp': fp, 'fn': fn}\n",
    "\n",
    "def format_value(v):\n",
    "    if isinstance(v, float):\n",
    "        return f'{v:.5f}'\n",
    "    else:\n",
    "        return str(v)\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(' '.join(f'{k}={format_value(v)}' for k, v in metrics.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(dets, scores, thresh):\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = scores\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_boxes(truth_boxes, truth_label, preds_center, preds_label):\n",
    "    assert isinstance(preds_label, np.ndarray)\n",
    "    tp = fp = fn = 0\n",
    "    # need to handle the same edge cases here as well\n",
    "    if truth_boxes.shape[0] == 0 or preds_center.shape[0] == 0:\n",
    "        fp += preds_center.shape[0]\n",
    "        fn += truth_boxes.shape[0]\n",
    "        return {'tp': tp, 'fp': fp, 'fn': fn}\n",
    "\n",
    "    preds_x = preds_center[:, 0]\n",
    "    preds_y = preds_center[:, 1]\n",
    "    truth_xmin, truth_ymin, truth_xmax, truth_ymax = truth_boxes.T\n",
    "    preds_unused = np.ones(len(preds_label)).astype(bool)\n",
    "    for xmin, xmax, ymin, ymax, label in zip(\n",
    "            truth_xmin, truth_xmax, truth_ymin, truth_ymax, truth_label):\n",
    "        # Matching = point inside box & character same &\n",
    "        # prediction not already used\n",
    "        matching = ((xmin < preds_x) & (xmax > preds_x) &\n",
    "                    (ymin < preds_y) & (ymax > preds_y) &\n",
    "                    (preds_label == label) & preds_unused)\n",
    "        if matching.sum() == 0:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tp += 1\n",
    "            preds_unused[np.argmax(matching)] = False\n",
    "    fp += preds_unused.sum()\n",
    "    return {'tp': tp, 'fp': fp, 'fn': fn}\n",
    "\n",
    "def to_coco(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" Convert from pytorch detection format to COCO format.\n",
    "    \"\"\"\n",
    "    boxes = boxes.clone()\n",
    "    boxes[:, 2] -= boxes[:, 0]\n",
    "    boxes[:, 3] -= boxes[:, 1]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def from_coco(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" Convert from CODO to pytorch detection format.\n",
    "    \"\"\"\n",
    "    boxes = boxes.clone()\n",
    "    boxes[:, 2] += boxes[:, 0]\n",
    "    boxes[:, 3] += boxes[:, 1]\n",
    "    return boxes\n",
    "\n",
    "def scale_boxes(\n",
    "        boxes: torch.Tensor, w_scale: float, h_scale: float) -> torch.Tensor:\n",
    "    return torch.stack([\n",
    "        boxes[:, 0] * w_scale,\n",
    "        boxes[:, 1] * h_scale,\n",
    "        boxes[:, 2] * w_scale,\n",
    "        boxes[:, 3] * h_scale,\n",
    "        ]).t()\n",
    "\n",
    "\n",
    "def submission_item(image_id, prediction):\n",
    "    return {\n",
    "        'image_id': image_id,\n",
    "        'labels': ' '.join(\n",
    "            ' '.join([p['cls']] +\n",
    "                     [str(int(round(v))) for v in p['center']])\n",
    "            for p in prediction),\n",
    "    }\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, output_dir, threshold):\n",
    "    cpu_device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    metric_logger = MetricLogger(delimiter='  ')\n",
    "    header = 'Test:'\n",
    "    scores = []\n",
    "    clf_gt = []\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        evaluator_time = time.time()\n",
    "        for target, image, output in zip(targets, images, outputs):\n",
    "            item = data_loader.dataset.df.iloc[target['idx'].item()]\n",
    "            del target\n",
    "            target_boxes, target_labels = get_target_boxes_labels(item)\n",
    "            target_boxes = torch.from_numpy(target_boxes)\n",
    "            boxes = output['boxes'][output['scores'] >= threshold]\n",
    "            boxes = to_coco(boxes)\n",
    "            with Image.open(get_image_path(\n",
    "                    item, data_loader.dataset.root)) as original_image:\n",
    "                ow, oh = original_image.size\n",
    "            _, h, w = image.shape\n",
    "            w_scale = ow / w\n",
    "            h_scale = oh / h\n",
    "            scaled_boxes = scale_boxes(boxes, w_scale, h_scale)\n",
    "            # If testing, scores is not important\n",
    "            scores.append(\n",
    "                dict(score_boxes(\n",
    "                    truth_boxes=from_coco(target_boxes).numpy(),\n",
    "                    truth_label=np.ones(target_labels.shape[0]),\n",
    "                    preds_center=torch.stack(\n",
    "                        [scaled_boxes[:, 0] + scaled_boxes[:, 2] * 0.5,\n",
    "                         scaled_boxes[:, 1] + scaled_boxes[:, 3] * 0.5]\n",
    "                    ).t().numpy(),\n",
    "                    preds_label=np.ones(boxes.shape[0]),\n",
    "                ), image_id=item.image_id))\n",
    "            clf_gt.append({\n",
    "                'labels': get_clf_gt(\n",
    "                    target_boxes=target_boxes,\n",
    "                    target_labels=target_labels,\n",
    "                    boxes=scaled_boxes),\n",
    "                'image_id': item.image_id,\n",
    "            })\n",
    "            if output_dir:\n",
    "                unscaled_target_boxes = scale_boxes(\n",
    "                    target_boxes, 1 / w_scale, 1 / h_scale)\n",
    "                _save_predictions(\n",
    "                    image, boxes, unscaled_target_boxes,\n",
    "                    path = output_dir +  '/{}.jpg'.format(item.image_id))\n",
    "\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(\n",
    "            model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print('Averaged stats:', metric_logger)\n",
    "    metrics = get_metrics(scores)\n",
    "    print_metrics(metrics)\n",
    "    \n",
    "    return metrics, (scores, clf_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEG_FP = 'unk'\n",
    "\n",
    "def get_clf_gt(target_boxes, target_labels, boxes, min_iou=0.5) -> str:\n",
    "    \"\"\" Create ground truth for classification from predicted boxes\n",
    "    in the same format as original ground truth, with addition of a class for\n",
    "    false negatives. Perform matching using box IoU.\n",
    "    \"\"\"\n",
    "    if boxes.shape[0] == 0:\n",
    "        return ''\n",
    "    if target_boxes.shape[0] == 0:\n",
    "        labels = [SEG_FP] * boxes.shape[0]\n",
    "    else:\n",
    "        ious = bbox_overlaps(from_coco(target_boxes).numpy(),\n",
    "                             from_coco(boxes).numpy())\n",
    "        ious_argmax = np.argmax(ious, axis=0)\n",
    "        assert ious_argmax.shape == (boxes.shape[0],)\n",
    "        labels = []\n",
    "        for k in range(boxes.shape[0]):\n",
    "            n = ious_argmax[k]\n",
    "            if ious[n, k] >= min_iou:\n",
    "                label = target_labels[n]\n",
    "            else:\n",
    "                label = SEG_FP\n",
    "            labels.append(label)\n",
    "    return ' '.join(\n",
    "        label + ' ' + ' '.join(str(int(round(float(x)))) for x in box)\n",
    "        for box, label in zip(boxes, labels))\n",
    "\n",
    "\n",
    "\n",
    "def bbox_overlaps(\n",
    "        bboxes1: np.ndarray, bboxes2: np.ndarray, mode='iou') -> np.ndarray:\n",
    "    \"\"\"Calculate the ious between each bbox of bboxes1 and bboxes2.\n",
    "\n",
    "    GH:open-mmlab/mmdetection/mmdet/core/evaluation/bbox_overlaps.py\n",
    "\n",
    "    Args:\n",
    "        bboxes1(ndarray): shape (n, 4)\n",
    "        bboxes2(ndarray): shape (k, 4)\n",
    "        mode(str): iou (intersection over union) or iof (intersection\n",
    "            over foreground)\n",
    "\n",
    "    Returns:\n",
    "        ious(ndarray): shape (n, k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert mode in ['iou', 'iof']\n",
    "\n",
    "    bboxes1 = bboxes1.astype(np.float32)\n",
    "    bboxes2 = bboxes2.astype(np.float32)\n",
    "    rows = bboxes1.shape[0]\n",
    "    cols = bboxes2.shape[0]\n",
    "    ious = np.zeros((rows, cols), dtype=np.float32)\n",
    "    if rows * cols == 0:\n",
    "        return ious\n",
    "    exchange = False\n",
    "    if bboxes1.shape[0] > bboxes2.shape[0]:\n",
    "        bboxes1, bboxes2 = bboxes2, bboxes1\n",
    "        ious = np.zeros((cols, rows), dtype=np.float32)\n",
    "        exchange = True\n",
    "    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n",
    "        bboxes1[:, 3] - bboxes1[:, 1] + 1)\n",
    "    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n",
    "        bboxes2[:, 3] - bboxes2[:, 1] + 1)\n",
    "    for i in range(bboxes1.shape[0]):\n",
    "        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n",
    "        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n",
    "        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n",
    "        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n",
    "        overlap = np.maximum(x_end - x_start + 1, 0) * np.maximum(\n",
    "            y_end - y_start + 1, 0)\n",
    "        if mode == 'iou':\n",
    "            union = area1[i] + area2 - overlap\n",
    "        else:\n",
    "            union = area1[i] if not exchange else area2\n",
    "        ious[i, :] = overlap / union\n",
    "    if exchange:\n",
    "        ious = ious.T\n",
    "    return ious\n",
    "\n",
    "BOX_COLOR = (255, 0, 0)\n",
    "def visualize_box(image: np.ndarray, bbox, color=BOX_COLOR, thickness=2):\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = \\\n",
    "        int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max),\n",
    "                  color=color, thickness=thickness)\n",
    "\n",
    "\n",
    "def visualize_boxes(image: np.ndarray, boxes, **kwargs):\n",
    "    image = image.copy()\n",
    "    for idx, bbox in enumerate(boxes):\n",
    "        visualize_box(image, bbox, **kwargs)\n",
    "    return image\n",
    "\n",
    "def _save_predictions(image, boxes, target, path: Path):\n",
    "    image = (image.detach().cpu() * 255).to(torch.uint8)\n",
    "    image = np.rollaxis(image.numpy(), 0, 3)\n",
    "    image = visualize_boxes(image, boxes, thickness=3)\n",
    "    image = visualize_boxes(image, target, color=(0, 255, 0), thickness=2)\n",
    "    Image.fromarray(image).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'input/train_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and word\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image max size 1685 px\n",
      "Epoch: [0]  [  0/194]  eta: 0:11:08  lr: 0.000031  loss: 0.1749 (0.1749)  loss_classifier: 0.0959 (0.0959)  loss_box_reg: 0.0521 (0.0521)  loss_objectness: 0.0103 (0.0103)  loss_rpn_box_reg: 0.0165 (0.0165)  time: 3.4466  data: 2.0547  max mem: 7099\n",
      "Epoch: [0]  [ 10/194]  eta: 0:10:33  lr: 0.000290  loss: 0.2128 (0.2064)  loss_classifier: 0.1112 (0.1066)  loss_box_reg: 0.0700 (0.0682)  loss_objectness: 0.0090 (0.0092)  loss_rpn_box_reg: 0.0227 (0.0224)  time: 3.4453  data: 2.0649  max mem: 7387\n",
      "Epoch: [0]  [ 20/194]  eta: 0:09:54  lr: 0.000548  loss: 0.2176 (0.2069)  loss_classifier: 0.1112 (0.1057)  loss_box_reg: 0.0700 (0.0692)  loss_objectness: 0.0088 (0.0090)  loss_rpn_box_reg: 0.0235 (0.0231)  time: 3.4159  data: 2.0366  max mem: 7387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-43023695a7c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;31m# update the learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-49d06dc649c0>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarmup_lr_scheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-d8bb7e5be7df>\u001b[0m in \u001b[0;36mlog_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    150\u001b[0m             ])\n\u001b[0;32m    151\u001b[0m         \u001b[0mMB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1024.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\henry\\anaconda3\\envs\\cuda10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\henry\\anaconda3\\envs\\cuda10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\henry\\anaconda3\\envs\\cuda10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-19d21fe95c83>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_image_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-19d21fe95c83>\u001b[0m in \u001b[0;36mread_image\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = Dataset(\n",
    "        df_train, get_transform(train=True), root, skip_empty=False)\n",
    "\n",
    "dataset_val = Dataset(\n",
    "        df_val, get_transform(train=False), root, skip_empty=False)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=16, shuffle=True, num_workers=0,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    #evaluate on the test dataset\n",
    "    metrics, (scores, clf_gt) = evaluate(model, data_loader_val, device=device, output_dir='./submissions/',\n",
    "            threshold=0.3)\n",
    "\n",
    "torch.save(model.state_dict(), 'models/detection/segmentation.pth')\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/detection/segmentation.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): ModelTransform()\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (7): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (7): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (8): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (9): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (10): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (11): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (12): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (13): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (14): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (15): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (16): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (17): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (18): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (19): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (20): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (21): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (22): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (23): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (24): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (25): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (26): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (27): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (28): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (29): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (30): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (31): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (32): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (33): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (34): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (35): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root = 'input/test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image max size 1685 px\n",
      "Test:  [   0/4150]  eta: 0:34:41  model_time: 0.2453 (0.2453)  evaluator_time: 0.1147 (0.1147)  time: 0.5016  data: 0.1296  max mem: 7387\n",
      "Test:  [ 100/4150]  eta: 0:33:39  model_time: 0.2289 (0.2250)  evaluator_time: 0.1152 (0.1123)  time: 0.4997  data: 0.1469  max mem: 7387\n",
      "Test:  [ 200/4150]  eta: 0:32:54  model_time: 0.2244 (0.2245)  evaluator_time: 0.1147 (0.1124)  time: 0.5151  data: 0.1654  max mem: 7387\n",
      "Test:  [ 300/4150]  eta: 0:32:26  model_time: 0.2284 (0.2240)  evaluator_time: 0.1137 (0.1127)  time: 0.5190  data: 0.1609  max mem: 7387\n",
      "Test:  [ 400/4150]  eta: 0:31:42  model_time: 0.2224 (0.2236)  evaluator_time: 0.1137 (0.1126)  time: 0.5124  data: 0.1650  max mem: 7387\n",
      "Test:  [ 500/4150]  eta: 0:30:59  model_time: 0.2224 (0.2237)  evaluator_time: 0.1147 (0.1125)  time: 0.5231  data: 0.1714  max mem: 7387\n",
      "Test:  [ 600/4150]  eta: 0:30:15  model_time: 0.2254 (0.2239)  evaluator_time: 0.1127 (0.1126)  time: 0.5147  data: 0.1662  max mem: 7387\n",
      "Test:  [ 700/4150]  eta: 0:29:26  model_time: 0.2224 (0.2237)  evaluator_time: 0.1177 (0.1129)  time: 0.5106  data: 0.1630  max mem: 7387\n",
      "Test:  [ 800/4150]  eta: 0:28:41  model_time: 0.2244 (0.2238)  evaluator_time: 0.1162 (0.1136)  time: 0.5162  data: 0.1612  max mem: 7387\n",
      "Test:  [ 900/4150]  eta: 0:27:50  model_time: 0.2219 (0.2236)  evaluator_time: 0.1127 (0.1133)  time: 0.5111  data: 0.1649  max mem: 7387\n",
      "Test:  [1000/4150]  eta: 0:27:01  model_time: 0.2264 (0.2238)  evaluator_time: 0.1171 (0.1135)  time: 0.5235  data: 0.1678  max mem: 7387\n",
      "Test:  [1100/4150]  eta: 0:26:07  model_time: 0.2179 (0.2234)  evaluator_time: 0.1072 (0.1132)  time: 0.4941  data: 0.1582  max mem: 7387\n",
      "Test:  [1200/4150]  eta: 0:25:15  model_time: 0.2184 (0.2232)  evaluator_time: 0.1107 (0.1133)  time: 0.5102  data: 0.1638  max mem: 7387\n",
      "Test:  [1300/4150]  eta: 0:24:22  model_time: 0.2174 (0.2228)  evaluator_time: 0.1102 (0.1130)  time: 0.4964  data: 0.1596  max mem: 7387\n",
      "Test:  [1400/4150]  eta: 0:23:29  model_time: 0.2174 (0.2224)  evaluator_time: 0.1112 (0.1128)  time: 0.5044  data: 0.1665  max mem: 7387\n",
      "Test:  [1500/4150]  eta: 0:22:36  model_time: 0.2199 (0.2221)  evaluator_time: 0.1107 (0.1126)  time: 0.5000  data: 0.1573  max mem: 7387\n",
      "Test:  [1600/4150]  eta: 0:21:44  model_time: 0.2184 (0.2219)  evaluator_time: 0.1077 (0.1125)  time: 0.4921  data: 0.1517  max mem: 7387\n",
      "Test:  [1700/4150]  eta: 0:20:51  model_time: 0.2159 (0.2216)  evaluator_time: 0.1107 (0.1124)  time: 0.5005  data: 0.1623  max mem: 7387\n",
      "Test:  [1800/4150]  eta: 0:19:58  model_time: 0.2164 (0.2214)  evaluator_time: 0.1137 (0.1122)  time: 0.5014  data: 0.1590  max mem: 7387\n",
      "Test:  [1900/4150]  eta: 0:19:06  model_time: 0.2154 (0.2212)  evaluator_time: 0.1087 (0.1121)  time: 0.5031  data: 0.1694  max mem: 7387\n",
      "Test:  [2000/4150]  eta: 0:18:14  model_time: 0.2159 (0.2210)  evaluator_time: 0.1097 (0.1119)  time: 0.4955  data: 0.1595  max mem: 7387\n",
      "Test:  [2100/4150]  eta: 0:17:22  model_time: 0.2124 (0.2208)  evaluator_time: 0.1067 (0.1118)  time: 0.4898  data: 0.1594  max mem: 7387\n",
      "Test:  [2200/4150]  eta: 0:16:31  model_time: 0.2194 (0.2206)  evaluator_time: 0.1077 (0.1116)  time: 0.4991  data: 0.1609  max mem: 7387\n",
      "Test:  [2300/4150]  eta: 0:15:39  model_time: 0.2174 (0.2204)  evaluator_time: 0.1117 (0.1115)  time: 0.5012  data: 0.1616  max mem: 7387\n",
      "Test:  [2400/4150]  eta: 0:14:48  model_time: 0.2174 (0.2203)  evaluator_time: 0.1067 (0.1114)  time: 0.5030  data: 0.1692  max mem: 7387\n",
      "Test:  [2500/4150]  eta: 0:13:56  model_time: 0.2170 (0.2201)  evaluator_time: 0.1157 (0.1113)  time: 0.5030  data: 0.1602  max mem: 7387\n",
      "Test:  [2600/4150]  eta: 0:13:05  model_time: 0.2174 (0.2200)  evaluator_time: 0.1082 (0.1113)  time: 0.4931  data: 0.1530  max mem: 7387\n",
      "Test:  [2700/4150]  eta: 0:12:14  model_time: 0.2163 (0.2199)  evaluator_time: 0.1087 (0.1112)  time: 0.4973  data: 0.1623  max mem: 7387\n",
      "Test:  [2800/4150]  eta: 0:11:23  model_time: 0.2184 (0.2197)  evaluator_time: 0.1121 (0.1111)  time: 0.5037  data: 0.1624  max mem: 7387\n",
      "Test:  [2900/4150]  eta: 0:10:31  model_time: 0.2164 (0.2196)  evaluator_time: 0.1067 (0.1110)  time: 0.4965  data: 0.1611  max mem: 7387\n",
      "Test:  [3000/4150]  eta: 0:09:41  model_time: 0.2188 (0.2195)  evaluator_time: 0.1057 (0.1108)  time: 0.4915  data: 0.1546  max mem: 7387\n",
      "Test:  [3100/4150]  eta: 0:08:50  model_time: 0.2174 (0.2195)  evaluator_time: 0.1127 (0.1108)  time: 0.5082  data: 0.1655  max mem: 7387\n",
      "Test:  [3200/4150]  eta: 0:07:59  model_time: 0.2164 (0.2194)  evaluator_time: 0.1117 (0.1107)  time: 0.4995  data: 0.1581  max mem: 7387\n",
      "Test:  [3300/4150]  eta: 0:07:08  model_time: 0.2194 (0.2193)  evaluator_time: 0.1112 (0.1107)  time: 0.4991  data: 0.1583  max mem: 7387\n",
      "Test:  [3400/4150]  eta: 0:06:18  model_time: 0.2164 (0.2192)  evaluator_time: 0.1047 (0.1106)  time: 0.4928  data: 0.1622  max mem: 7387\n",
      "Test:  [3500/4150]  eta: 0:05:27  model_time: 0.2194 (0.2192)  evaluator_time: 0.1047 (0.1105)  time: 0.5019  data: 0.1639  max mem: 7387\n",
      "Test:  [3600/4150]  eta: 0:04:37  model_time: 0.2204 (0.2191)  evaluator_time: 0.1102 (0.1105)  time: 0.4938  data: 0.1524  max mem: 7387\n",
      "Test:  [3700/4150]  eta: 0:03:46  model_time: 0.2140 (0.2190)  evaluator_time: 0.1070 (0.1104)  time: 0.4881  data: 0.1552  max mem: 7387\n",
      "Test:  [3800/4150]  eta: 0:02:56  model_time: 0.2149 (0.2189)  evaluator_time: 0.1087 (0.1104)  time: 0.4959  data: 0.1604  max mem: 7387\n",
      "Test:  [3900/4150]  eta: 0:02:05  model_time: 0.2154 (0.2188)  evaluator_time: 0.1097 (0.1104)  time: 0.4949  data: 0.1611  max mem: 7387\n",
      "Test:  [4000/4150]  eta: 0:01:15  model_time: 0.2124 (0.2186)  evaluator_time: 0.1067 (0.1104)  time: 0.4915  data: 0.1617  max mem: 7387\n",
      "Test:  [4100/4150]  eta: 0:00:25  model_time: 0.2115 (0.2185)  evaluator_time: 0.1047 (0.1103)  time: 0.4853  data: 0.1566  max mem: 7387\n",
      "Test:  [4149/4150]  eta: 0:00:00  model_time: 0.2134 (0.2184)  evaluator_time: 0.1042 (0.1103)  time: 0.4803  data: 0.1516  max mem: 7387\n",
      "Test: Total time: 0:34:45 (0.5026 s / it)\n",
      "Averaged stats: model_time: 0.2134 (0.2184)  evaluator_time: 0.1042 (0.1103)\n",
      "f1=0.00000 tp=0 fp=938260 fn=0\n"
     ]
    }
   ],
   "source": [
    "df_test['labels'] = ''\n",
    "\n",
    "dataset_test = Dataset(\n",
    "        df_test, get_transform(train=False), test_root, skip_empty=False)\n",
    "\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "_, (_, pred_results) = evaluate(\n",
    "            model, data_loader_test, device=device, output_dir='./submissions/',\n",
    "            threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 'unk 1132 1693 91 124 unk 1456 1619 96 108 unk 1444 2583 96 151 unk 800 1066 100 112 unk 1605 1502 82 133 unk 1457 2342 73 106 unk 1298 2088 73 81 unk 1439 1161 94 79 unk 973 1865 84 115 unk 977 803 88 115 unk 1443 1320 92 117 unk 972 1012 92 95 unk 1276 2440 91 139 unk 182 1862 62 95 unk 338 1540 70 74 unk 1598 1292 87 137 unk 1610 1427 66 63 unk 177 2177 88 125 unk 1149 1609 65 85 unk 816 987 77 69 unk 1600 1634 77 97 unk 1458 1247 71 73 unk 964 1668 112 111 unk 799 2573 96 82 unk 1296 933 63 73 unk 1143 1407 67 70 unk 973 1773 78 94 unk 976 2075 88 117 unk 172 2069 83 114 unk 1446 1540 95 79 unk 645 1316 94 78 unk 987 934 67 73 unk 1293 1011 94 82 unk 497 2770 72 76 unk 1455 1930 80 129 unk 1162 2254 38 115 unk 982 1996 70 83 unk 813 803 86 112 unk 816 2657 66 111 unk 814 2774 75 84 unk 1480 1819 50 112 unk 500 2112 82 74 unk 1473 2053 48 105 unk 660 2766 61 85 unk 180 2539 53 97 unk 1292 1733 89 97 unk 1294 2179 72 73 unk 662 2411 64 124 unk 500 747 65 60 unk 338 1949 60 106 unk 500 1140 64 92 unk 828 2032 43 102 unk 482 1876 83 86 unk 1142 1008 67 82 unk 808 1521 82 65 unk 829 1913 41 130 unk 991 1231 52 111 unk 330 959 75 75 unk 658 2700 73 58 unk 337 873 85 79 unk 659 969 69 105 unk 1444 988 116 89 unk 496 2656 64 89 unk 1465 2732 61 117 unk 654 2288 74 120 unk 350 2188 53 127 unk 821 1823 61 81 unk 1145 1091 58 124 unk 815 2141 68 91 unk 330 1859 49 92 unk 1448 1439 106 99 unk 1610 1161 68 136 unk 1316 1092 32 79 unk 342 2765 82 88 unk 1300 2320 69 120 unk 658 1916 54 95 unk 185 749 56 73 unk 501 2188 81 157 unk 806 1587 99 79 unk 513 1240 40 116 unk 1281 738 75 72 unk 354 1035 50 118 unk 985 1101 74 128 unk 1299 797 52 120 unk 348 1608 45 119 unk 1132 2761 75 96 unk 663 1078 67 79 unk 804 1176 76 130 unk 1295 2251 70 68 unk 356 1151 41 105 unk 988 1345 59 149 unk 825 1309 48 107 unk 657 1613 63 96 unk 496 2386 67 115 unk 1306 1179 70 103 unk 1315 1494 47 106 unk 823 1411 59 108 unk 981 2721 63 134 unk 497 1363 89 109 unk 1149 1210 41 88 unk 972 2627 71 91 unk 172 1557 75 114 unk 1127 1476 94 133 unk 1463 2153 78 72 unk 1144 1304 68 106 unk 658 1540 71 75 unk 1470 1083 46 75 unk 495 1688 64 69 unk 510 812 50 90 unk 496 1585 67 115 unk 1124 2469 89 110 unk 666 2544 59 73 unk 156 914 84 87 unk 502 906 71 64 unk 329 1267 87 129 unk 995 2322 32 83 unk 195 1092 28 99 unk 675 741 44 102 unk 1477 2226 39 120 unk 656 2105 78 118 unk 161 1708 79 157 unk 995 2183 42 139 unk 356 2309 51 100 unk 1315 1600 39 127 unk 1288 1819 85 93 unk 494 1479 72 104 unk 1158 2030 62 96 unk 1291 2694 49 65 unk 178 2640 53 98 unk 1490 1729 27 93 unk 191 2386 33 78 unk 838 914 27 79 unk 168 1977 87 83 unk 172 1475 58 81 unk 338 2426 74 55 unk 347 1729 41 123 unk 1155 1817 38 112 unk 331 2675 74 64 unk 497 2496 66 100 unk 521 1059 33 87 unk 980 2412 68 107 unk 1149 2368 61 109 unk 831 1665 38 74 unk 1297 2747 62 108 unk 327 2065 99 114 unk 508 2352 54 32 unk 665 2014 82 75 unk 798 2387 103 105 unk 1326 1920 24 160 unk 678 852 26 121 unk 1157 914 33 96 unk 665 1390 44 107 unk 497 2581 67 66 unk 970 1491 100 129 unk 514 959 35 91 unk 1148 2578 59 70 unk 519 2045 43 66 unk 826 2268 36 122 unk 675 1197 25 98 unk 1013 2517 20 103 unk 357 744 29 96 unk 692 2204 25 110 unk 1169 2118 21 129 unk 1132 1939 73 105 unk 196 2319 42 64 unk 979 1599 74 69 unk 833 741 36 68 unk 539 1936 21 106 unk 364 2474 41 80 unk 178 1201 59 113 unk 192 2750 30 97 unk 1302 2583 59 33 unk 643 1708 83 64 unk 1165 807 24 105 unk 501 1757 61 125 unk 1159 2646 31 109 unk 646 1496 87 64 unk 1440 737 95 177 unk 1457 2474 59 109 unk 345 1435 79 98 unk 707 1145 15 108 unk 667 1764 47 80 unk 690 2614 16 78 unk 1316 1388 43 104 unk 206 1028 20 68 unk 1452 879 67 121 unk 197 975 43 56 unk 1329 1270 29 105 unk 838 1738 26 85 unk 171 1426 52 57 unk 1465 2450 74 36 unk 196 2462 22 76 unk 1167 732 25 73 unk 187 858 43 64 unk 174 1326 49 104 unk 814 2464 66 114 unk 1604 894 68 52 unk 975 744 72 15 unk 367 2555 32 124 unk 669 1834 28 82 unk 1307 2609 34 85 unk 187 823 45 45 unk 344 1388 41 90',\n",
       " 'image_id': 'test_001c37e2'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./detection_output'):\n",
    "    os.mkdir('./detection_output')\n",
    "\n",
    "pd.DataFrame(pred_results).to_csv( 'detection_output/detected.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
